## Neural Networks: Zero to Hero

This repository documents my journey implementing neural networks from scratch, drawing inspiration from Andrej Karpathy's excellent course.

Here, I've implemented:

- [Micrograd](https://github.com/karpathy/micrograd): A tiny autograd engine.
- [Makemore](https://github.com/karpathy/makemore): A character-level language model.
- [Causal Dilated Convolution](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html): Implemented a causal dilated convolutional layer in PyTorch.
- [Wavenet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/): A character-level language model that predicts the next character based on 8 characters, implemented using PyTorch modules and self-written Torch modules.
- [DenseNet](https://arxiv.org/abs/1608.06993): A dense convolutional neural network implemented using PyTorch modules.
- [ResNet](https://arxiv.org/abs/1512.03385): A residual convolutional neural network implemented using PyTorch modules.
- [NanoGPT](https://github.com/karpathy/nanogpt): A minimal GPT model built using PyTorch modules, focusing exclusively on the pretraining stage.
  - [Attention is All You Need Transformer](https://arxiv.org/abs/1706.03762): A Transformer model implementation that emphasizes self-attention for sequence modeling.
  - Exploring the Transformer model and its components.
