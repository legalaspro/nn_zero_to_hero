{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laDyGmNsQHyR"
      },
      "source": [
        "## Building a GPT\n",
        "\n",
        "1. Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rP1OjX2QTmC",
        "outputId": "1c30f064-2ea7-40a7-b713-a32caeeb6b36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-02-12 22:23:12--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2025-02-12 22:23:13 (162 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzIuMI6TQa8v",
        "outputId": "66d247db-fd5c-4b6f-cd45-fb974f8af413"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num Parameters: 10788929\n",
            "iter 0, train loss: 4.2849, valid loss: 4.2823\n",
            "elapsed time: 15.05 seconds\n",
            "iter 500, train loss: 2.0040, valid loss: 2.0911\n",
            "elapsed time: 86.04 seconds\n",
            "iter 1000, train loss: 1.5949, valid loss: 1.7782\n",
            "elapsed time: 156.52 seconds\n",
            "iter 1500, train loss: 1.4385, valid loss: 1.6410\n",
            "elapsed time: 227.03 seconds\n",
            "iter 2000, train loss: 1.3406, valid loss: 1.5710\n",
            "elapsed time: 297.53 seconds\n",
            "iter 2500, train loss: 1.2795, valid loss: 1.5348\n",
            "elapsed time: 368.04 seconds\n",
            "iter 3000, train loss: 1.2272, valid loss: 1.5110\n",
            "elapsed time: 438.53 seconds\n",
            "iter 3500, train loss: 1.1826, valid loss: 1.4935\n",
            "elapsed time: 509.02 seconds\n",
            "iter 4000, train loss: 1.1465, valid loss: 1.4901\n",
            "elapsed time: 579.57 seconds\n",
            "iter 4500, train loss: 1.1092, valid loss: 1.4771\n",
            "elapsed time: 650.08 seconds\n",
            "iter 4999, train loss: 1.0763, valid loss: 1.4903\n",
            "elapsed time: 720.52 seconds\n",
            "Training done!\n",
            "elapsed time: 720.57 seconds\n",
            "Model saved as 'bigram_model.pt'.\n",
            "\n",
            "Had you to proved to my sistal lawful boy:\n",
            "Right abhors and one me: shall I have so,\n",
            "That hear my father like die the gave money--\n",
            "Heart himself mine honour'. Saize, die! trump your\n",
            "He will'd fourteen your complots, Waminollies\n",
            "Were  without and tie howmage of your hands,--\n",
            "And I am you poor empt of the redender,\n",
            "I am delivered and send it doth school;\n",
            "But a person being of man alterous joy\n",
            "Can their mover: they were a monister offence,\n",
            "Some discenteditation of fools rights: as they we stand die\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences to train at once in parallel\n",
        "block_size = 256  # what is maximum context length for predictions\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")  # A100 ~ 2.75x faster then ( ~ 11.8 min with 10mln parameters)\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\") #  ~ 8.35x faster than CPU (~ 32.5 min with 10mln parameters)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6 # 384/6 = 64\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "#---------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# here all the unique characters in the file\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a dictionary mapping characters to integers\n",
        "stoi = {s:i for i,s in enumerate(chars)}\n",
        "itos = {i:s for i,s in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: Take in string, return list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: Take in list of integers, return string\n",
        "\n",
        "# Train and Test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.int64)\n",
        "n = int(0.9 * len(data)) # first 90% of the data will be training data, rest will be validation data\n",
        "train_data, valid_data = data[:n], data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data inputs x and targets y\n",
        "    data = train_data if split == 'train' else valid_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # starting index for each sequence\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]) # input data\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # target data\n",
        "    x, y = x.to(device), y.to(device) # move data to device\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estiamte_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'valid']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# self attention head\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x) # (B, T, H)\n",
        "        q = self.query(x) # (B, T, H)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, H) @ (B, H, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei) # randomly prevent some tokens to communicate with each other\n",
        "        # perform the weighted aggregation of values\n",
        "        v = self.value(x) # (B, T, H)\n",
        "        out = wei @ v # (B,T,T) @ (B, T, H) -> (B, T, H)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd) # num_heads * head_size = n_embd\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out)) # projection back to original pathway (residual x+f(x)) + information mixing from all heads\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a nonlinearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        # position wise feed forward network\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # expand the dimensionality by a factor of 4 - expansion layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # this is projection layer back to original pathway - contraction layer\n",
        "            nn.Dropout(dropout) # dropout layer - right before connection back to original pathway (residual x+f(x))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# Now we want blocks where tokens communicate with each other and then process themselves\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation/processing \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        \"\"\"Initializer for the Block class\n",
        "\n",
        "        Args:\n",
        "            n_embd: embedding dimension\n",
        "            n_head: the number of heads we'd like\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(num_heads=n_head, head_size=head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # by adding the input to the output of the self-attention and feed forward layers,\n",
        "        # we are allowing the model to learn residual connections like in ResNet\n",
        "        x = x + self.sa(self.ln1(x)) # communication\n",
        "        x = x + self.ffwd(self.ln2(x)) # computation/processing\n",
        "        return x\n",
        "\n",
        "\n",
        "#super simple Bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        #self.sa_heads = MultiHeadAttention(num_heads=4, head_size=n_embd//4) # i.e. 4 heads of 8-dimensional self-attention (32/4)\n",
        "        # self.blocks = nn.Sequential(\n",
        "        #     Block(n_embd, n_head=4),\n",
        "        #     Block(n_embd, n_head=4),\n",
        "        #     Block(n_embd, n_head=4),\n",
        "        #     nn.LayerNorm(n_embd)\n",
        "        # )\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B, T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, C) - batch size, time steps, channels(n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B, T, C)\n",
        "        # x = self.sa_heads(x) # (B, T, C) apply one head of self-attention. # (so here tokens communicate with each other)!!\n",
        "        # x = self.ffwd(x) # (B, T, C) apply feed forward layer # (and here each token is processing itself, what it learned from other tokens)!!!\n",
        "        x = self.blocks(x) # (B, T, C) apply blocks of self-attention and feed forward, where tokens communicate with each other and then process themselves\n",
        "        x = self.ln_f(x) # (B, T, C)\n",
        "        logits = self.lm_head(x) # (B, T, V) - batch size, time steps, vocab size\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B* T, C)\n",
        "            targets = targets.view(-1) # flatten the targets B*T\n",
        "            loss = F.cross_entropy(logits, targets) # cross entropy needs input as (B, C, T) and target as (B, T), where T can be many dim, but C needs to be 2nd dim\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is a (B, T) tensor of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self.forward(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            id_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled token to the running sequence\n",
        "            idx = torch.cat([idx, id_next], dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "print(f'Num Parameters: {sum(p.numel() for p in model.parameters())}')\n",
        "\n",
        "\n",
        "# create a torch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    #every once in a while, evaluate the loss on train and validation data\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estiamte_loss()\n",
        "        print(f'iter {iter}, train loss: {losses[\"train\"]:.4f}, valid loss: {losses[\"valid\"]:.4f}')\n",
        "        print(f'elapsed time: {time.time() - start_time:.2f} seconds')\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "     # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print('Training done!')\n",
        "print(f'elapsed time: {time.time() - start_time:.2f} seconds')\n",
        "\n",
        "# Save the model's state dict\n",
        "torch.save(model.state_dict(), 'bigram_model.pt')\n",
        "print(\"Model saved as 'bigram_model.pt'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlAuQmYrQsZH",
        "outputId": "6cf3609e-b37c-4519-bc9b-38ad7b475c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "And no less broken, grave and prayers,\n",
            "Till be consorted with princess them,\n",
            "For they gives hom go. Suff'st forgive the place\n",
            "Which does with sensel; Warwick triple our court,\n",
            "Which, say'st thou hast promised me with blood\n",
            "Marriage upon thy less' bless of health,\n",
            "He hath forst pruck'd against;\n",
            "And, if you hadven plead better, he determy,\n",
            "Swear he hands, as he does, his lovours, looks as it speech\n",
            "That soundeth thus, for the Duke of Rome,\n",
            "So brave a pair and most old SomeREdio Ross indeed;\n",
            "And le\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, 500).squeeze().tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXqpWGpzSxZt",
        "outputId": "b87c6706-c555-4950-a7cf-4e6a029c097e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from 'bigram_model.pt'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-f842e76fecdf>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('bigram_model.pt', map_location=device))\n"
          ]
        }
      ],
      "source": [
        "# To load the model (for later use or inference), you can use:\n",
        "model = BigramLanguageModel()\n",
        "model.load_state_dict(torch.load('bigram_model.pt', map_location=device))\n",
        "model = model.to(device)\n",
        "print(\"Model loaded from 'bigram_model.pt'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVK5csPoTbgJ",
        "outputId": "7795042f-2c9f-4a5f-cd9b-1a71f5e48c30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text saved to generated_text.txt\n"
          ]
        }
      ],
      "source": [
        "# generate from the model and save to a file\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_text = decode(model.generate(context, 10000).squeeze().tolist())\n",
        "\n",
        "with open(\"generated_text.txt\", \"w\") as file:\n",
        "    file.write(generated_text)\n",
        "\n",
        "print(\"Generated text saved to generated_text.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Hhvsxw9T0JT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
